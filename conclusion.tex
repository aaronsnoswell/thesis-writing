
\chapter{Conclusion}
\label{chap:conclusion}

Light field cameras are a revolutionary new form of imaging that enable many unconventional photographic techniques.
While these cameras have seen applications in a range of areas, their use in robotics has not yet been fully realised.
Unfortunately, some robotic navigation and computer vision systems suffer from the presence of motion blur in images.

State of the art deconvolution de-blurring techniques have the major drawback that they only work for scenes with a constant depth.
Robotic systems, especially ground or underwater mobile robots, typically operate in environments where the constant depth approximation is invalid.
For this reason, a means for de-blurring images at all scene depths is desirable.
Deriving a so-called \enquote*{depth-aware deonvolution} method was the focus of our research.

By analysing the math describing projective camera geometry an equation describing the formation of motion blur in a translating camera was derived (Equation \ref{eq:blur_width_simple}).
This equation was verified experimentally (Figure \ref{fig:blur_vs_depth}), indicating that calibrated depth data can be combined with camera intrinsic and trajectory information to perform depth-aware deconvolution.

It was found experimentally that Lytro depth maps can be calibrated to metric units using an empirically-determined piecewise linear mapping (Equation  \ref{eq:lytro_dm_to_metric}), provided a rough scene depth is known.

An experiment was performed to test the simplified case of linear camera motion blur at short distances.
Depth-aware deconvolution was compared with traditional de-blurring methods for three scenes using household objects.
The results showed that our method resulted in more high-frequency content and less ringing noise than 2D deconvolution methods.
It was found our method's performance was on-par with 2D deconvolution in scenes with little scene structure, due to the adverse effect this has on depth map estimation.

Our research made the assumption of linear camera motion, and relied on a rough scene depth estimate being supplied.
Additionally, our experiments were performed in a controlled environment.
Many areas for future work were identified, including adapting our method to generalised camera trajectories, utilising light field confidence information, collection of further data to confirm our results and the possibility of optimising our code to try and achieve real-time operation.
Perhaps the most interesting area to explore in the future is the possibility of using blind deconvolution methods to recover camera velocity whilst simultaneously de-blurring the light field.

Light field cameras have a host of potential applications in robotic systems.
It is hoped that our research and work demonstrating the feasibility of depth-aware deconvolution will enable further use of this exciting new technology.
