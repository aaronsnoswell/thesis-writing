
\chapter{Results and Discussion}
\label{chap:results_and_discussion}

\section{Experimental Results}
\label{sec:experimental_results}

Using the indoor motion platform described in Section \ref{sec:development_of_linear_motion_platform} and the experimental setup described in Section \ref{sec:scene_design}, images from three individual scenes with depths ranging up to \nicetilde0.4m were captured.
The motion platform velocity was controlled and was measured at $v_c = 0.021m/s$, the Lytro exposure time was varied from 1/6.4s to 1/2s and the Lytro ISO setting set to automatic to maintain scene brightness.

The Lytro depth maps were calibrated using the shallow depth range mapping described in Figure \ref{fig:depth_real_world_vs_lytro_response}.
The depth map data was then used to de-blur the light fields, using the technique described in Section \ref{sec:image_processing_and_deconvolution}.
For comparison, the light field images were also de-blurred using the Richardson-Lucy method, blind deconvolution and a simple regularized deconvolution implementation, all with a single PSF computed at the median scene depth.

Light field images from the three scenes are shown in Figures XXXX to XXXX, along with the respective depth maps, our results, and results from 2D deconvolution.

[Figures XXXX to XXXX here showing the LF images from the three scenes and de-blurred results]

The Q factor and 


\section{Limitations}
\label{sec:limitations}
- linear motion only
- blur in a controlled environment
- camera velocity/trajectory was supplied
- more data required to relate LF camera data to real-world depth, but it seems like it has some real world correlation at this point
- at this point the deblurring is an offline process, reducing its current usefulness for robotics
- deblurring is computationally intensive and needs to be optimised
- LF depth-confidence data hasn't been used, if utalised these could improve the process
- although many images were captured and a number of different deblurring methods were applied before conclusions were reached only 3 photos have been utalised to evaluate final results
- computationally slow 
- not viable for mobile robotics yet


\section{Implications}
\label{sec:implications}
- motion blur has the potential has the potential to adversely affect robotics systems that use vision
- given a compact RGB-depth sensor it is possible to deblur motion-blurred images simultaneously at all depths
- practical applications for increasing consumer satisfaction of hand-held LF devices
- theoretically people care about blurred pictures more than robots
- enable robots to sharpen LF pictures where required and utalise depth data for the scene around it
- improve LF depth information by reducing blur - reduced blur = improved depth info
- noise/ringing-artifacts reduce the everyday usefulness of the photo, may increase the use for forensic photography - text recover etc


\section{Future Work}
\label{sec:future_work}
- integrate LF-confidence and depth-confidence into systems
- rotational blur 
- generalised camera trajectories
- ways to integrate a LF camera into a robotic platform without requiring a desktop computer - depth data could be used instantaneously
- different LF shapes eg. more angular resolution
- generalised scenes with large distances 
- better quality depth estimation
- better characterisation of the depth data
- explore blind-deconvolution in conjunction with depth-data, removing the requirement for extra information
- repeat on large scale with a greater number of images
- LF Depth Map quality drops when motion blur is present (show house sequence of depth maps). This suggests that the method presented here could be adapted to an iterative form (depth->deblur->depth again->deblur again).
- LF Depth confidence map was not used at all - this could be used to improve the de-blurring process. E.g. in regions of low depth confidence, all candidate de-blurred pixels are compared in a local neighborhood and the sharpest pixels are selected.
- Generalise to camera rotation or arbitrary camera trajectory
- Generalise to object motion (local de-blurring vs. global deblurring)
- Explore with other shapes of LF (e.g. more angular resolution)
- Future Work and Extensions - Very last item in discussion

