
\chapter{Background}

\section{Light Field Theory}

%Side Note
%The first practical work using light fields dates back to a photographic technique known as ‘Integral Photography’, developed by Gabriel Lipmann in 1908.
%Lipmann’s technique involved placing an array of lenses over a piece of film.
%The film was then exposed to a scene, encoding 3D information through the lens pattern. 
%By reversing this process (projecting light through the film, then through an identical lens array), the first 3D pictures were created.
%Modern techniques allow Lipmann’s lens arrays to be printed on a surface, creating what is commonly known as ‘holographic pictures’ that morph and change as the viewer’s perspective changes.
%GIVE MORE INFORMATION ON INTEGRAL PHOTOGRAPHY.
%The ability to create a 3D display in this manner (without extra viewing devices such as 3D glasses) is known as auto-stereoscopic imaging.


The light field is a mathematical description of the way light propagates.
Another name for this is the ‘Plenoptic Function', from the Latin ‘plenus’, meaning full, and ‘optic’, meaning light.
The plenoptic function describes every possible configuration a light ray could ever be in, encompassing the three spatial dimensions, two angular dimensions, temporal changes as well as frequency (colour), phase and polarisation changes.
In most computer vision literature the phase and polarisation elements are ignored, leading to a 7-dimensional definition of the plenoptic function;

\begin{equation}
\label{eq:plenoptic_function}
P = P(V_x, V_y, V_z, \theta, \phi, t, \lambda)
\end{equation}

where $V_x$, $V_y$ and $V_z$ are the spatial dimensions, $\theta$ and $\phi$ are the angular dimensions, $t$ is the temporal dimension and $\lambda$ describes the colour of the ray. 

The plenoptic function in this form was first described by Adelson and Bergen in 1991 \cite{adelson1991plenoptic}.
Adelson and Bergen developed a light field theory by asking the fundamental question \enquote{What can be seen?} They stressed that the plenoptic function is the sole visual communication link between physical objects and the eye.
They also suggest that the function of \enquote*{early vision} in biological and artificial organisms is to sample the derivative of this function along various dimensions such as \enquote*{change in colour with x} or \enquote*{change in brightness with time}.
GIVE DETAIL ABOUT SAMPLING PLENOPTIC FUNCTION EXAMPLES FROM ADELSON PAPER.

In modern computer vision literature a number of parameterisations are used to describe the plenoptic function.
These different representations each provide new insights into the information contained in the light field.

LIGHT SLAB DESCRIPTION AND DISCUSSION.

LIGHT TUBE DESCRIPTION AND DISCUSSION.

POLAR AND/OR DUAL PLANE DESCRIPTION AND DISCUSSION?


Traditional cameras generally sample the Light Field along two spatial dimensions (x and y pixel locations) and in the colour dimension (normally with Red, Green and Blue photon collectors), however there are many instances in which sampling more dimensions of the light field would be beneficial. To this end, in 1996 two light field papers were included in the Siggraph conference proceedings; ‘Light Field Rendering’ by Levoy et al. and ‘The Lumigraph’ by Gortler et al [REFERENCES]. Both these papers described independent research into means for capturing more information from the plenoptic function. Importantly, both groups made the observation that, while the full plenoptic function is seven dimensional, for a region of space free of occluding objects, one space dimension can be disregarded, and the 3D structure of the scene still fully recovered. GO INTO MORE DETAIL ABOUT OCCLUSION FREE SPACE SIMPLIFICATION HERE. This approximation is the basis for most modern light field cameras sampling in four dimensions, and paved the way for the creation of the first practical light field cameras.

All physical cameras (or eyes) can only ever observe a small subset (various ‘slices’) of the ambient light field. As an example, consider the human visual system. The human visual system takes several million samples from the angular dimensions, three points from the lambda dimension (roughly corresponding to the frequencies known as ‘Blue’, ‘Green’ and ‘Red’) and only two samples from the Vx dimension (one each for the left and right eyes). In addition to this, the distribution of sample points is not always linear. The cone cells responsible for detecting colour in the human eye are much more populous in the fovetal region near the centre of the eye, while rod cells that operate in low-light conditions are clustered around the outer edges of the retina.

The work done by Gortler and Levoy (et al.) demonstrated it was possible to capture 4D light field information using a traditional camera sensor with some modifications. While there is active research in developing angle-sensitive pixels, modern light field cameras typically encode angular ray information by mapping ray angles to locations on a traditional 2D camera sensor. Thus, regular spatial resolution is directly traded to gain angular resolution.

For example the Lytro light field camera used in this paper has an 11 mega-pixel sensor chip, however a spatial and angular resolution of only 1080x1080 and 10x10 respectively. The trade-off between angular resolution and spatial resolution is unavoidable when using this technique, and the choice of angular and spatial resolutions limit the functions that can be performed with the captured light field. GIVE EXMPLES OF HOW NOT ALL LIGHT FIELDS ARE EQUAL. DISCUSS THE BENEFITS/DRAWBACKS OF THE LYTRO CAMERA CONFIGURATION. 4D light field sampling has also been performed using multiple traditional cameras separated spatially in a grid, or separated temporally.

GO INTO MORE DETAIL ABOUT LYTRO CAMRERA AND BRIEFLY COMPARE LYTRO WITH RAYTRIX AND OTHER LF CAMERAS (STANFORD MULTI CAMERA ARRAY). ALSO DISCUSS SYNTHETIC LIGHT FIELD CREATION AND GIVE EXMAPLES. HIGHLIGHT BENEFITS AND DRAWBACKS OF SYNTHETIC LIGHT FIELDS VS REAL TEST DATA. MENTION THE APPEARANCE OF LIGHT FIELD CAMERAS ON MOBILE PHONES.

Mapping of ray angles to the 2D camera plane is typically done by placing either a lenslet or mirror array, or a cosine pinhole mask within the light ray path. These approaches are functionally equivalent, however each has disadvantages and benefits. A lenslet or mirror array is di cult to machine and requires custom hardware and precision positioning. On the other hand, a pinhole array can be added to some modern cameras with little modification, however blocks a large majority of the incoming light, thus requiring longer exposure times and increasing the likelihood of motion blurring.

2 OR 3 PARAGRAPHS ABOUT LOW RESOLUTION AND LIGHT FIELD SUPER RESOLUTION PAPER.

POSSIBLY 2-3 MORE PARAGRAPHS ABOUT RECENT LIGHT FIELD RESEARCH AND DEVELOPMENTS.

2 SUMMARISING PARAGRAPHS BRINGING ALL OF THIS INFORMATION BACK TO MY THESIS – HIGHLIGHTING WHAT PARTS I’M GOING TO BE USING, WHAT LF SYMBOLS AND CONVENTIONS I’LL BE USING ETC.



\section{Motion Blur Formation}


\begin{equation}
\label{eq:camera_projection}
k
\begin{bmatrix}
x \\
y \\
1 \\
\end{bmatrix}
=
\begin{bmatrix}
\alpha && s && u_0 \\
0 && \beta && v_0 \\
0 && 0 && 1 \\
\end{bmatrix}
\begin{bmatrix}
r_{11} && r_{12} && r_{13} && t_x \\
r_{21} && r_{22} && r_{23} && t_y \\
r_{31} && r_{32} && r_{33} && t_z \\
\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
Z \\
1
\end{bmatrix}
\end{equation}


\begin{equation}
\label{eq:blur_width_full}
w = \left( \frac{\alpha + s + u_0}{Z} \right) v_c t_e
\end{equation}


\begin{equation}
\label{eq:blur_width_simple}
w = \frac{f}{Z} v_c t_e
\end{equation}


\section{Image De-blurring}

Image formation

Sources of degradation during image formation:
 - Optical blur (light diffraction through lens system), Rayleigh criterion
 - Noise (quantum statistical behaviour of light (shot noise) and read noise (electronics / detection device)).
 - Motion induced blur

Image formation in the fourier domain
 - g(x) = f(x) * h(x) -> G(w) = F(w) x H(w)
 - Additive noise

Point Spread Function – Optical system response to a punctual light source
Optical Transfer Function – Fourier transform of the PSF. Describes optical system behaviour in the fourier domain

Simple deconvolution approaches
 - Inverse \& Regularized Inverse filter
 -- Noise amplification
 - Wiener Filter
 --Signal to Noise Ratio

Iterative Deconvolution
 - Least squares estimation (+regularized)
 - Maximum likelihood estimation (Richardson-Lucy)

Unsharp Masking

Blind deconvolution

